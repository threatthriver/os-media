2025-04-04 18:31:48,081 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 18:31:48,081 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:31:48,081 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 18:31:48,081 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:31:48,081 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 18:32:44,442 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 18:32:44,443 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Max Steps: 10
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 18:32:44,447 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 18:32:47,990 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 18:32:47,991 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 18:32:47,991 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 18:32:47,991 - LLM-Trainer - INFO - Initializing tokenizer...
2025-04-04 18:32:48,427 - LLM-Trainer - INFO - Initializing 7b model...
2025-04-04 18:32:48,427 - LLM-Trainer - ERROR - Error during training: type object 'FlaxAutoModelForCausalLM' has no attribute 'config_class'
2025-04-04 18:32:48,428 - LLM-Trainer - ERROR - Traceback (most recent call last):
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 344, in train_model
    FlaxAutoModelForCausalLM.config_class(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'FlaxAutoModelForCausalLM' has no attribute 'config_class'

2025-04-04 18:32:48,433 - LLM-Trainer - ERROR - Training failed
2025-04-04 18:33:24,404 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 18:33:24,404 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:33:24,404 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 18:33:24,404 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:33:24,404 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 18:34:19,453 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 18:34:19,453 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 18:34:19,457 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Max Steps: 10
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 18:34:19,458 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 18:34:21,384 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 18:34:21,384 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 18:34:21,384 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 18:34:21,384 - LLM-Trainer - INFO - Initializing tokenizer...
2025-04-04 18:34:21,874 - LLM-Trainer - INFO - Initializing 7b model...
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 18:51:16,004 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 18:51:16,004 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Max Steps: 5
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Model configuration:
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Parameters: 7 billion
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Hidden size: 4096
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Layers: 32
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Attention heads: 32
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Intermediate size: 11008
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Simulating model training...
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Starting training...
2025-04-04 18:51:16,050 - LLM-Trainer - INFO - Step 1/5 (20.0%)
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Elapsed: 0.01s, Remaining: 0.05s
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Steps/second: 79.90
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Loss: 2.0000
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Perplexity: 7.3891
2025-04-04 18:51:16,100 - LLM-Trainer - INFO - Step 5/5 (100.0%)
2025-04-04 18:51:16,100 - LLM-Trainer - INFO - Elapsed: 0.06s, Remaining: 0.00s
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Steps/second: 80.17
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Loss: 0.0000
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Perplexity: 1.0000
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Saving checkpoint at step 5...
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - Training completed in 0.06 seconds
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - Average steps/second: 77.76
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - ================================================================================
2025-04-04 18:51:16,102 - LLM-Trainer - INFO -                         Training Completed Successfully!                        
2025-04-04 18:51:16,103 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:20:53,752 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 23:20:53,752 - LLM-Trainer - ERROR - Unhandled exception: prepare_dataset() takes 1 positional argument but 2 were given
2025-04-04 23:20:53,752 - LLM-Trainer - ERROR - Traceback (most recent call last):
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 714, in <module>
    sys.exit(main())
             ~~~~^^
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 692, in main
    if not prepare_dataset(args.dataset, args.output_dir):
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: prepare_dataset() takes 1 positional argument but 2 were given

2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 23:24:13,434 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 23:24:13,435 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Max Steps: 5
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Using Flash Attention: True
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Using Reasoning Layer: True
2025-04-04 23:24:13,468 - LLM-Trainer - WARNING - ⚠️ No TPU devices found: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache). Available backends are ['cpu']
2025-04-04 23:24:13,468 - LLM-Trainer - WARNING - Training will proceed on available devices, but will be much slower without TPU
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Default backend: cpu
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Model configuration:
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Parameters: 7 billion
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Hidden size: 4096
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Layers: 32
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Attention heads: 32
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Intermediate size: 11008
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Creating 7b model with optimizations for coding tasks...
2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Error during training: create_model() got an unexpected keyword argument 'use_flash_attention'
2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Traceback (most recent call last):
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 412, in train_model
    llm = model.create_model(
        model_size=args.model_size,
    ...<2 lines>...
        use_reasoning_layer=args.use_reasoning_layer
    )
TypeError: create_model() got an unexpected keyword argument 'use_flash_attention'

2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Training failed
