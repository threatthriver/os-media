2025-04-04 18:50:09,297 - LLM-Trainer - INFO - ================================================================================
2025-04-04 18:50:09,298 - LLM-Trainer - INFO -                             LLM Training - 7b Model                             
2025-04-04 18:50:09,298 - LLM-Trainer - INFO - ================================================================================
2025-04-04 18:50:09,298 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:50:09,298 - LLM-Trainer - INFO - | Checking Dependencies
2025-04-04 18:50:09,298 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:50:09,298 - LLM-Trainer - INFO - Python version: 3.13.2
2025-04-04 18:50:09,740 - LLM-Trainer - INFO - ✓ jax is installed
2025-04-04 18:50:09,740 - LLM-Trainer - INFO - ✓ jaxlib is installed
2025-04-04 18:50:09,844 - LLM-Trainer - INFO - ✓ flax is installed
2025-04-04 18:50:09,903 - LLM-Trainer - INFO - ✓ optax is installed
2025-04-04 18:50:09,903 - LLM-Trainer - INFO - ✓ numpy is installed
2025-04-04 18:50:11,024 - LLM-Trainer - INFO - ✓ transformers is installed
2025-04-04 18:50:11,477 - datasets - INFO - PyTorch version 2.6.0 available.
2025-04-04 18:50:11,478 - datasets - INFO - JAX version 0.5.3 available.
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - ✓ datasets is installed
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - ✓ huggingface_hub is installed
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - All required dependencies are installed
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - | Checking System Resources
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - CPU Cores: 8
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Memory: 8.00 GB total, 2.26 GB available
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Disk: 228.27 GB total, 89.68 GB free
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Model Size: 7B parameters
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Estimated Memory Requirements:
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Model: 14.00 GB
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Optimizer: 28.00 GB
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Activations: 2.80 GB
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Total: 44.80 GB
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Estimated Disk Requirements:
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Per Checkpoint: 16.80 GB
2025-04-04 18:50:11,717 - LLM-Trainer - INFO -   - Total (5 checkpoints): 84.00 GB
2025-04-04 18:50:11,717 - LLM-Trainer - WARNING - ⚠️ Insufficient memory: 8.00 GB available, 44.80 GB required
2025-04-04 18:50:11,717 - LLM-Trainer - WARNING - Training may fail or be very slow due to memory constraints
2025-04-04 18:50:11,717 - LLM-Trainer - WARNING - Consider using a smaller model size or adding more memory
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - ✓ Disk space is sufficient for training
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - | Preparing Environment
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:50:11,717 - LLM-Trainer - INFO - Created output directory: ./output
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Created checkpoint directory: ./output/checkpoints
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Created logs directory: ./output/logs
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Created tensorboard directory: ./output/tensorboard
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:50:11,718 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 18:51:16,004 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 18:51:16,004 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 18:51:16,008 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Max Steps: 5
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 18:51:16,009 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 18:51:16,034 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2025-04-04 18:51:16,038 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache)
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Model configuration:
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Parameters: 7 billion
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Hidden size: 4096
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Layers: 32
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Attention heads: 32
2025-04-04 18:51:16,038 - LLM-Trainer - INFO -   - Intermediate size: 11008
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Simulating model training...
2025-04-04 18:51:16,038 - LLM-Trainer - INFO - Starting training...
2025-04-04 18:51:16,050 - LLM-Trainer - INFO - Step 1/5 (20.0%)
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Elapsed: 0.01s, Remaining: 0.05s
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Steps/second: 79.90
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Loss: 2.0000
2025-04-04 18:51:16,051 - LLM-Trainer - INFO - Perplexity: 7.3891
2025-04-04 18:51:16,100 - LLM-Trainer - INFO - Step 5/5 (100.0%)
2025-04-04 18:51:16,100 - LLM-Trainer - INFO - Elapsed: 0.06s, Remaining: 0.00s
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Steps/second: 80.17
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Loss: 0.0000
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Perplexity: 1.0000
2025-04-04 18:51:16,101 - LLM-Trainer - INFO - Saving checkpoint at step 5...
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - Training completed in 0.06 seconds
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - Average steps/second: 77.76
2025-04-04 18:51:16,102 - LLM-Trainer - INFO - ================================================================================
2025-04-04 18:51:16,102 - LLM-Trainer - INFO -                         Training Completed Successfully!                        
2025-04-04 18:51:16,103 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:20:49,189 - LLM-Trainer - INFO -                             LLM Training - 7b Model                             
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - | Checking Dependencies
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:20:49,189 - LLM-Trainer - INFO - Python version: 3.13.2
2025-04-04 23:20:49,611 - LLM-Trainer - INFO - ✓ jax is installed
2025-04-04 23:20:49,611 - LLM-Trainer - INFO - ✓ jaxlib is installed
2025-04-04 23:20:49,722 - LLM-Trainer - INFO - ✓ flax is installed
2025-04-04 23:20:49,786 - LLM-Trainer - INFO - ✓ optax is installed
2025-04-04 23:20:49,786 - LLM-Trainer - INFO - ✓ numpy is installed
2025-04-04 23:20:52,989 - LLM-Trainer - INFO - ✓ transformers is installed
2025-04-04 23:20:53,485 - datasets - INFO - PyTorch version 2.6.0 available.
2025-04-04 23:20:53,486 - datasets - INFO - JAX version 0.5.3 available.
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - ✓ datasets is installed
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - ✓ huggingface_hub is installed
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - All required dependencies are installed
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - | Checking System Resources
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - CPU Cores: 8
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Memory: 8.00 GB total, 2.29 GB available
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Disk: 228.27 GB total, 90.53 GB free
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Model Size: 7B parameters
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Estimated Memory Requirements:
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Model: 14.00 GB
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Optimizer: 28.00 GB
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Activations: 2.80 GB
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Total: 44.80 GB
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Estimated Disk Requirements:
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Per Checkpoint: 16.80 GB
2025-04-04 23:20:53,751 - LLM-Trainer - INFO -   - Total (5 checkpoints): 84.00 GB
2025-04-04 23:20:53,751 - LLM-Trainer - WARNING - ⚠️ Insufficient memory: 8.00 GB available, 44.80 GB required
2025-04-04 23:20:53,751 - LLM-Trainer - WARNING - Training may fail or be very slow due to memory constraints
2025-04-04 23:20:53,751 - LLM-Trainer - WARNING - Consider using a smaller model size or adding more memory
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - ✓ Disk space is sufficient for training
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - | Preparing Environment
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Created output directory: ./output
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Created checkpoint directory: ./output/checkpoints
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Created logs directory: ./output/logs
2025-04-04 23:20:53,751 - LLM-Trainer - INFO - Created tensorboard directory: ./output/tensorboard
2025-04-04 23:20:53,752 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 23:20:53,752 - LLM-Trainer - ERROR - Unhandled exception: prepare_dataset() takes 1 positional argument but 2 were given
2025-04-04 23:20:53,752 - LLM-Trainer - ERROR - Traceback (most recent call last):
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 714, in <module>
    sys.exit(main())
             ~~~~^^
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 692, in main
    if not prepare_dataset(args.dataset, args.output_dir):
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: prepare_dataset() takes 1 positional argument but 2 were given

2025-04-04 23:23:07,687 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:23:07,687 - LLM-Trainer - INFO -                             LLM Training - 7b Model                             
2025-04-04 23:23:07,687 - LLM-Trainer - INFO - ================================================================================
2025-04-04 23:23:07,687 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:23:07,687 - LLM-Trainer - INFO - | Checking Dependencies
2025-04-04 23:23:07,687 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:23:07,687 - LLM-Trainer - INFO - Python version: 3.13.2
2025-04-04 23:23:08,106 - LLM-Trainer - INFO - ✓ jax is installed
2025-04-04 23:23:08,106 - LLM-Trainer - INFO - ✓ jaxlib is installed
2025-04-04 23:23:08,211 - LLM-Trainer - INFO - ✓ flax is installed
2025-04-04 23:23:08,269 - LLM-Trainer - INFO - ✓ optax is installed
2025-04-04 23:23:08,269 - LLM-Trainer - INFO - ✓ numpy is installed
2025-04-04 23:23:09,437 - LLM-Trainer - INFO - ✓ transformers is installed
2025-04-04 23:23:09,871 - datasets - INFO - PyTorch version 2.6.0 available.
2025-04-04 23:23:09,872 - datasets - INFO - JAX version 0.5.3 available.
2025-04-04 23:23:10,094 - LLM-Trainer - INFO - ✓ datasets is installed
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - ✓ huggingface_hub is installed
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - All required dependencies are installed
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - | Checking System Resources
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - CPU Cores: 8
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Memory: 8.00 GB total, 2.41 GB available
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Disk: 228.27 GB total, 90.52 GB free
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Model Size: 7B parameters
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Estimated Memory Requirements:
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Model: 14.00 GB
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Optimizer: 28.00 GB
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Activations: 2.80 GB
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Total: 44.80 GB
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Estimated Disk Requirements:
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Per Checkpoint: 16.80 GB
2025-04-04 23:23:10,095 - LLM-Trainer - INFO -   - Total (5 checkpoints): 84.00 GB
2025-04-04 23:23:10,095 - LLM-Trainer - WARNING - ⚠️ Insufficient memory: 8.00 GB available, 44.80 GB required
2025-04-04 23:23:10,095 - LLM-Trainer - WARNING - Training may fail or be very slow due to memory constraints
2025-04-04 23:23:10,095 - LLM-Trainer - WARNING - Consider using a smaller model size or adding more memory
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - ✓ Disk space is sufficient for training
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - | Preparing Environment
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Created output directory: ./output
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Created checkpoint directory: ./output/checkpoints
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Created logs directory: ./output/logs
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Created tensorboard directory: ./output/tensorboard
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Environment prepared successfully
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - | Preparing Dataset: HuggingFaceFW/fineweb
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:23:10,095 - LLM-Trainer - INFO - Checking if dataset HuggingFaceFW/fineweb exists...
2025-04-04 23:24:13,434 - LLM-Trainer - INFO - Dataset HuggingFaceFW/fineweb found
2025-04-04 23:24:13,435 - LLM-Trainer - INFO - Dataset info: IterableDatasetDict({
    train: IterableDataset({
        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],
        num_shards: 25868
    })
})
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - 
--------------------------------------------------------------------------------
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - | Starting Model Training
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - --------------------------------------------------------------------------------
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Model Size: 7b
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Dataset: HuggingFaceFW/fineweb
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Batch Size: 32
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Learning Rate: 0.00015
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Max Steps: 5
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Max Sequence Length: 131072
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Output Directory: ./output
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Using Flash Attention: True
2025-04-04 23:24:13,439 - LLM-Trainer - INFO - Using Reasoning Layer: True
2025-04-04 23:24:13,465 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2025-04-04 23:24:13,468 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache)
2025-04-04 23:24:13,468 - LLM-Trainer - WARNING - ⚠️ No TPU devices found: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache). Available backends are ['cpu']
2025-04-04 23:24:13,468 - LLM-Trainer - WARNING - Training will proceed on available devices, but will be much slower without TPU
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - JAX devices: [CpuDevice(id=0)]
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Number of devices: 1
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Default backend: cpu
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Creating model configuration...
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Model configuration:
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Parameters: 7 billion
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Hidden size: 4096
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Layers: 32
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Attention heads: 32
2025-04-04 23:24:13,468 - LLM-Trainer - INFO -   - Intermediate size: 11008
2025-04-04 23:24:13,468 - LLM-Trainer - INFO - Creating 7b model with optimizations for coding tasks...
2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Error during training: create_model() got an unexpected keyword argument 'use_flash_attention'
2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Traceback (most recent call last):
  File "/Users/aniketkumar/Documents/temp-project/run.py", line 412, in train_model
    llm = model.create_model(
        model_size=args.model_size,
    ...<2 lines>...
        use_reasoning_layer=args.use_reasoning_layer
    )
TypeError: create_model() got an unexpected keyword argument 'use_flash_attention'

2025-04-04 23:24:13,469 - LLM-Trainer - ERROR - Training failed
